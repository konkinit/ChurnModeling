{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/ikonkobo/Desktop/Self_Learning/telco_churn/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reading\n",
    "The raw data are stored in the storage system S3 through SSP cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df = pd.read_csv(\"./data/raw/commsdata.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        b'Customer service is no longer an option with...\n",
       "1        b'My phone was stollen 5 days ago  I got the n...\n",
       "2        b'MTT needs to lower their plans    Im conside...\n",
       "3        b'Phones were turned off due to past due bill ...\n",
       "4                                          b'It was great'\n",
       "                               ...                        \n",
       "56552    b'Great customer service and very helpful  Tha...\n",
       "56553    b'Acqueline fantastic great personality answer...\n",
       "56554     b'americans serving americans always works best'\n",
       "56555       b'Wish we had LTE or faster 4g in Olympia  WA'\n",
       "56556    b'It was very frustrating when I was disconnec...\n",
       "Name: verbatims, Length: 56557, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comms_df[\"verbatims\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = comms_df[\"verbatims\"].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decoding str is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_[\u001b[39m\"\u001b[39m\u001b[39mverbatims\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df_[\u001b[39m\"\u001b[39;49m\u001b[39mverbatims\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x : \u001b[39mstr\u001b[39;49m(x, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m)) \n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/series.py:4357\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4247\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4248\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4249\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4252\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4253\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FrameOrSeriesUnion:\n\u001b[1;32m   4254\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4255\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4256\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4355\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4356\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4357\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py:1043\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1040\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m-> 1043\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py:1098\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   1093\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1099\u001b[0m             values,\n\u001b[1;32m   1100\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1101\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1102\u001b[0m         )\n\u001b[1;32m   1104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1105\u001b[0m     \u001b[39m# GH 25959 use pd.array instead of tolist\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m     \u001b[39m# so extension arrays can be used\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(pd_array(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2859\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_[\u001b[39m\"\u001b[39m\u001b[39mverbatims\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df_[\u001b[39m\"\u001b[39m\u001b[39mverbatims\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x : \u001b[39mstr\u001b[39;49m(x, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m)) \n",
      "\u001b[0;31mTypeError\u001b[0m: decoding str is not supported"
     ]
    }
   ],
   "source": [
    "df_[\"verbatims\"] = df_[\"verbatims\"].apply(lambda x : str(x, encoding='utf-8')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_[\"verbatims_process\"] = df_[\"verbatims\"].apply(lambda x : re.sub(\"[\\,.?!]\", \"\", x))\n",
    "df_[\"verbatims_process\"] = df_[\"verbatims_process\"].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "data = df_.verbatims_process.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "#print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# number of topics\n",
    "num_topics = 15\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "# Print the Keyword in the 10 topics\n",
    "# pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_model.get_topic_terms(5))\n",
    "topic_word = [id2word[term[0]] for term in lda_model.get_topic_terms(5)]     \n",
    "topic = '\"'+','.join(topic_word)+'\"'\n",
    "topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topic(ldamodel, topic):\n",
    "    topic_word = [id2word[term[0]] for term in ldamodel.get_topic_terms(topic)]     \n",
    "    return '\"'+','.join(topic_word)+'\"'\n",
    "\n",
    "list_topics = [extract_topic(lda_model, i) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df__ = comms_df.select_dtypes(exclude='float64')\n",
    "#df__\n",
    "df__.drop([\"verbatims\", \"resolution\"], axis=1, inplace=True)\n",
    "comms_df.drop(list(df__.columns), axis=1, inplace=True)\n",
    "df_ = pd.get_dummies(df__, prefix_sep=\"_\", drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df[df_.columns] = df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(set([word for doc in comms_df[\"verbatims\"].values for word in word_tokenize(doc.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df___ = comms_df[\"verbatims\"].to_frame()\n",
    "df___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "       df___[word] = df___[\"verbatims\"].apply(lambda x: x.count(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df[\"verbatims\"].values[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df__.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "The project focuses on churn prediction and the dataset has 2 potential target variables which are \n",
    "+ *churn* : indicates wheteher customer churned \n",
    "+ *upsell_xsell* : indicates customer’s flag for cross-sell or up-sell. \n",
    "Since we focus on churn prediction, *upsell_xsell* will be deleted. \n",
    "\n",
    "Furthermore, the variables listed below are useless for predictive modeling and will be rejected :\n",
    "+ city\n",
    "+ city_lat\n",
    "+ city_long\n",
    "+ data_usage_amt\n",
    "+ mou_onnet_6m_normal\n",
    "+ mou_roam_6m_normal\n",
    "+ region_lat\n",
    "+ region_long\n",
    "+ state_lat\n",
    "+ state_long\n",
    "+ tweedie_adjusted\n",
    "\n",
    "In addition, we notice that character variables are showed in this format b'prime'. We will suppress characters 'b' and '."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df.drop([\"upsell_xsell\", \"city\", \"city_lat\", \"city_long\", \"data_usage_amt\", \"mou_onnet_6m_normal\", \"mou_roam_6m_normal\", \"region_lat\",\n",
    "\"region_long\", \"state_lat\", \"state_long\", \"tweedie_adjusted\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_vars_object = list(comms_df.select_dtypes(exclude = ['int64', 'float64']).columns)\n",
    "\n",
    "for var in list_vars_object:\n",
    "    comms_df[var] = comms_df[var].apply(lambda x : x.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_mqtes = comms_df.isnull().sum().to_frame().reset_index()\n",
    "df_val_mqtes.columns = [\"variable\", \"nb_valeur_manquante\"]\n",
    "df_val_mqtes = df_val_mqtes[df_val_mqtes.nb_valeur_manquante > 0].reset_index(drop=True)\n",
    "df_val_mqtes['pourcent_valeur_manquante'] = round(100 * df_val_mqtes['nb_valeur_manquante'] / comms_df.shape[0], 2)\n",
    "df_val_mqtes = df_val_mqtes.sort_values('nb_valeur_manquante', ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_val_mqtes.variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_ = [var for var in list(df_val_mqtes.variable) if len(comms_df[var].unique()) < 50]\n",
    "\n",
    "l_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df[\"tot_drpd_pr1\"].value_counts(ascending=False).to_frame().reset_index().iloc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(comms_df[df_val_mqtes.variable[4]])#, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df.churn.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable churn seems to not have missing values. The dataset contains 12.13 % of customers churned. We will split the dataset to 70 % for training and 30% for test using *churn* as startify variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = comms_df.drop(columns=[\"churn\"])\n",
    "y = comms_df.churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_train \n",
    "train[\"churn\"] = y_train\n",
    "\n",
    "test = X_test \n",
    "test[\"churn\"] = y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
